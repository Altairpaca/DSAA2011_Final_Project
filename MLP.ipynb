{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b70a2-a1d7-48a6-899c-47ba6049ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             roc_curve, auc, make_scorer, f1_score)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier  # 核心MLP实现\n",
    "from sklearn.exceptions import ConvergenceWarning      # 处理收敛警告\n",
    "import warnings                                        # 警告管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08649883-48fe-4ded-b69c-72f15a21d84c",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6061caf-935b-4ec4-8b6c-01fa4c636b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Keep the first row, which are the titles\n",
    "X_train_df = pd.read_csv('X_train.csv', header=0)\n",
    "X_test_df = pd.read_csv('X_test.csv', header=0)\n",
    "X_train = X_train_df.values  # Convert to numpy array\n",
    "X_test = X_test_df.values\n",
    "# Extract Series from single-column DataFrame\n",
    "y_train = pd.read_csv('y_train.csv', header=0).squeeze()\n",
    "y_test = pd.read_csv(\"y_test.csv\", header=0).squeeze()\n",
    "\n",
    "# Save the list of feature names\n",
    "feature_columns = X_train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2a8ad-eb29-49f7-a3fc-ba72b0e8282a",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25e875-2348-41d9-b77f-403410770413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the label to start from 0 to meet the requirement of XGBoost\n",
    "min_label = min(y_train.min(), y_test.min())\n",
    "if min_label > 0:\n",
    "    y_train -= min_label\n",
    "    y_test -= min_label\n",
    "    print(f\"Adjusted the label to start from 0: {np.unique(y_train)}\")\n",
    "\n",
    "# Standardize to improve the model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f070f-5d66-468b-9a41-7a8612a6d6db",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "##### Model Configuration Update & Enhanced Training Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b28e5-2b88-479e-8da1-1ee62b965873",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    'LogisticRegression': {\n",
    "        'class': LogisticRegression,\n",
    "        'params': {'max_iter': 1000, 'class_weight': 'balanced'}\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'class': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 200, 'max_depth': 10, 'class_weight': 'balanced'}\n",
    "    },\n",
    "    'SVM': {\n",
    "        'class': SVC,\n",
    "        'params': {'kernel': 'rbf', 'C': 1.0, 'class_weight': 'balanced'}\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'class': XGBClassifier,\n",
    "        'params': {'eval_metric': 'logloss'}\n",
    "    },\n",
    "    'MLP': {\n",
    "        'class': MLPClassifier,\n",
    "        'params': {\n",
    "            'hidden_layer_sizes': (100, 50),  # Two hidden layers\n",
    "            'activation': 'relu',\n",
    "            'solver': 'adam',\n",
    "            'early_stopping': True,\n",
    "            'max_iter': 500,\n",
    "            'learning_rate_init': 0.001,\n",
    "            'batch_size': 256\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    trained_models = {}\n",
    "    for model_name, config in MODEL_CONFIG.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # model initialization\n",
    "        model = config['class'](**config['params'])\n",
    "\n",
    "        # Special handling for MLP convergence warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            model.fit(X_train, y_train)\n",
    "        # Track loss curve for neural networks\n",
    "        loss_curve = None\n",
    "        if hasattr(model, 'loss_curve_'):\n",
    "            loss_curve = model.loss_curve_\n",
    "\n",
    "        # record the model date\n",
    "        trained_models[model_name] = {\n",
    "            'model': model,\n",
    "            'train_time': time.time() - start_time,\n",
    "            'feature_importances': getattr(model, 'feature_importances_', None),\n",
    "            'loss_curve': loss_curve\n",
    "        }\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "trained_models = train_models(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27620b0d-4979-489f-95f0-68166aad955b",
   "metadata": {},
   "source": [
    "### 4. Neural Network-Specific Analysis\n",
    "##### Add diagnostic visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa0a93-3895-49e5-8b97-5b39270263ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural_network_diagnostics(model_data):\n",
    "    if model_data['loss_curve'] is not None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(model_data['loss_curve'])\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "    # Visualize layer weights\n",
    "    if isinstance(model_data['model'], MLPClassifier):\n",
    "        weights = model_data['model'].coefs_\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, layer in enumerate(weights[:-1]):  # Exclude output layer\n",
    "            plt.subplot(1, len(weights)-1, i+1)\n",
    "            sns.heatmap(layer, cmap='viridis')\n",
    "            plt.title(f'Layer {i+1} Weights')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def predict_all_sets(model, X_train, X_test):\n",
    "    # Test the trained model on the training set, testing set and the entire set\n",
    "    X_full = np.vstack([X_train, X_test])\n",
    "    return {\n",
    "        'train': model.predict(X_train),\n",
    "        'test': model.predict(X_test),\n",
    "        'full': model.predict(X_full)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name, model_name):\n",
    "    # Generate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "    plt.title(f'{model_name} - {dataset_name} Set Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compare_models(results):\n",
    "    comparison = pd.DataFrame()\n",
    "    for model_name, data in results.items():\n",
    "        metrics = {\n",
    "            'Train Accuracy': data['train']['accuracy'],\n",
    "            'Test Accuracy': data['test']['accuracy'],\n",
    "            'Full Accuracy': data['full']['accuracy'],\n",
    "            'Train Precision': data['train']['precision'],\n",
    "            'Test Precision': data['test']['precision'],\n",
    "            'Full Precision': data['full']['precision'],\n",
    "            'Train Recall': data['train']['recall'],\n",
    "            'Test Recall': data['test']['recall'],\n",
    "            'Full Recall': data['full']['recall'],\n",
    "            'Train F1': data['train']['f1'],\n",
    "            'Test F1': data['test']['f1'],\n",
    "            'Full F1': data['full']['f1']\n",
    "        }\n",
    "        comparison[model_name] = pd.Series(metrics)\n",
    "    return comparison.T\n",
    "\n",
    "\n",
    "# Generate neural network diagnostics\n",
    "for name, data in trained_models.items():\n",
    "    if name == 'MLP':\n",
    "        plot_neural_network_diagnostics(data)\n",
    "\n",
    "# Prediction and evaluation flow\n",
    "results = {}\n",
    "for model_name, model_data in trained_models.items():\n",
    "    # Get predictions using scaled data\n",
    "    preds = predict_all_sets(\n",
    "        model_data['model'], X_train_scaled, X_test_scaled)\n",
    "\n",
    "    # Create full dataset labels\n",
    "    y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "    # Evaluate all sets with visualization\n",
    "    results[model_name] = {\n",
    "        'train': evaluate_model(y_train, preds['train'], 'Train', model_name),\n",
    "        'test': evaluate_model(y_test, preds['test'], 'Test', model_name),\n",
    "        'full': evaluate_model(y_full, preds['full'], 'Full', model_name)\n",
    "    }\n",
    "\n",
    "# Generate comparison table with full set metrics\n",
    "comparison_table = compare_models(results)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "# ROC and AUC\n",
    "# Modify SVM configuration to enable probability estimates\n",
    "MODEL_CONFIG['SVM']['params']['probability'] = True\n",
    "\n",
    "# Function to plot ROC curves and compute AUC for each model\n",
    "\n",
    "\n",
    "def plot_roc_and_calculate_auc(trained_models, X_test_scaled, y_test):\n",
    "    for model_name, model_data in trained_models.items():\n",
    "        model = model_data['model']\n",
    "\n",
    "        # Skip models without probability estimates\n",
    "        if not hasattr(model, 'predict_proba'):\n",
    "            print(\n",
    "                f\"Skipping ROC/AUC for {model_name} (no probability support)\")\n",
    "            continue\n",
    "\n",
    "        # Get model's class information\n",
    "        model_classes = model.classes_\n",
    "        n_classes = len(model_classes)\n",
    "\n",
    "        # Binarize test labels according to model's classes\n",
    "        y_test_bin = label_binarize(y_test, classes=model_classes)\n",
    "\n",
    "        # Get predicted probabilities\n",
    "        y_prob = model.predict_proba(X_test_scaled)\n",
    "\n",
    "        # Initialize structures for ROC/AUC\n",
    "        fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "        # Calculate metrics per class\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute macro-average\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Compute micro-average\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(\n",
    "            y_test_bin.ravel(), y_prob.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))\n",
    "\n",
    "        # Plot individual classes\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                     label=f'Class {model_classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "        # Plot averages\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.2f})',\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.2f})',\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "        # Formatting\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'Multi-class ROC for {model_name}')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"\\n{model_name} AUC Summary:\")\n",
    "        for i in range(n_classes):\n",
    "            print(f\"  Class {model_classes[i]}: {roc_auc[i]:.2f}\")\n",
    "        print(f\"  Macro-Average: {roc_auc['macro']:.2f}\")\n",
    "        print(f\"  Micro-Average: {roc_auc['micro']:.2f}\")\n",
    "\n",
    "\n",
    "# Execute ROC/AUC analysis\n",
    "plot_roc_and_calculate_auc(trained_models, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43d354-c6b2-4236-8ba5-2376e02bec14",
   "metadata": {},
   "source": [
    "### 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd50c7-ecaf-48e4-b3d1-0e8d8df02b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix feature_columns definition (correct the typo)\n",
    "feature_columns = X_train_df.columns.tolist()\n",
    "\n",
    "# Visualize Decision Boundaries using PCA\n",
    "\n",
    "\n",
    "def plot_decision_boundaries(trained_models, X_train, X_test, y_train):\n",
    "    # Combine train and test for full PCA fit\n",
    "    X_full = np.vstack([X_train, X_test])\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X_full)\n",
    "\n",
    "    # Transform both train and test\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    for model_name, model_data in trained_models.items():\n",
    "        model = model_data['model']\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Predict on PCA meshgrid\n",
    "        Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot decision boundary\n",
    "        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "        # Plot training data points\n",
    "        scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train,\n",
    "                              edgecolor='k', cmap=plt.cm.RdYlBu, s=50)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.title(f'Decision Boundary for {model_name}\\n(PCA Projection)')\n",
    "        plt.legend(*scatter.legend_elements(), title='Classes')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize Feature Importances\n",
    "\n",
    "\n",
    "def plot_feature_importances(trained_models, feature_columns):\n",
    "    for model_name, model_data in trained_models.items():\n",
    "        importances = model_data['feature_importances']\n",
    "        if importances is not None:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
    "            plt.title(f\"Feature Importances - {model_name}\")\n",
    "            plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "            plt.xticks(range(len(indices)), [feature_columns[i] for i in indices],\n",
    "                       rotation=45, ha='right')\n",
    "            plt.xlim([-0.5, len(indices)-0.5])\n",
    "            plt.ylabel(\"Importance Score\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Execute visualization functions\n",
    "print(\"\\nGenerating Visualizations...\")\n",
    "plot_decision_boundaries(trained_models, X_train, X_test, y_train)\n",
    "plot_feature_importances(trained_models, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4cd3d-8593-49da-8f32-6da1322d6323",
   "metadata": {},
   "source": [
    "### 6. Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863041a-12a3-4454-9a0a-dddd36134ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    'MLP': {\n",
    "        'class': MLPClassifier,\n",
    "        'params': {\n",
    "            'hidden_layer_sizes': (50,),\n",
    "            'activation': 'relu',\n",
    "            'solver': 'adam',\n",
    "            'alpha': 0.0001,\n",
    "            'batch_size': 128,\n",
    "            'early_stopping': True,\n",
    "            'max_iter': 500\n",
    "        },\n",
    "        'grid_params': {  # 新增网格搜索专用参数\n",
    "            'hidden_layer_sizes': [(50,), (100, 50), (200, 100, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'batch_size': [64, 128, 256],\n",
    "            'learning_rate_init': [0.001, 0.01],\n",
    "            'early_stopping': [True]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ================== 可视化函数 ==================\n",
    "\n",
    "\n",
    "def plot_grid_search_results(model_data, model_name):\n",
    "    \"\"\"可视化网格搜索结果的3D参数空间\"\"\"\n",
    "    results = model_data['grid_results']\n",
    "\n",
    "    # 转换结果为DataFrame\n",
    "    df = pd.DataFrame(results.cv_results_)\n",
    "\n",
    "    # 选择关键参数进行可视化\n",
    "    viz_params = ['param_alpha', 'param_batch_size',\n",
    "                  'param_learning_rate_init']\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # 3D散点图\n",
    "    ax = plt.subplot(221, projection='3d')\n",
    "    sc = ax.scatter3D(df[viz_params[0]], df[viz_params[1]], df[viz_params[2]],\n",
    "                      c=df['mean_test_score'], cmap='viridis', s=100)\n",
    "    ax.set_xlabel(viz_params[0].split('_')[-1])\n",
    "    ax.set_ylabel(viz_params[1].split('_')[-1])\n",
    "    ax.set_zlabel(viz_params[2].split('_')[-1])\n",
    "    plt.colorbar(sc, label='Accuracy')\n",
    "\n",
    "    # 热力图（alpha vs batch_size）\n",
    "    plt.subplot(223)\n",
    "    pivot_df = df.pivot_table(values='mean_test_score',\n",
    "                              index='param_alpha',\n",
    "                              columns='param_batch_size')\n",
    "    sns.heatmap(pivot_df, annot=True, fmt=\".3f\")\n",
    "\n",
    "    # 时间-精度曲线\n",
    "    plt.subplot(224)\n",
    "    plt.scatter(df['mean_fit_time'], df['mean_test_score'], c=df['param_learning_rate_init'],\n",
    "                cmap='coolwarm')\n",
    "    plt.xlabel('Training Time (s)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.colorbar(label='Learning Rate')\n",
    "\n",
    "    plt.suptitle(f'{model_name} Grid Search Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ================== 网格搜索优化函数 ==================\n",
    "\n",
    "\n",
    "def improve_models_via_gridsearch(trained_models, MODEL_CONFIG, X_train, y_train):\n",
    "    \"\"\"执行网格搜索优化流程\"\"\"\n",
    "    improved_models = {}\n",
    "\n",
    "    for model_name, base_model in trained_models.items():\n",
    "        if model_name not in MODEL_CONFIG:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Optimizing {model_name} ===\")\n",
    "        config = MODEL_CONFIG[model_name]\n",
    "\n",
    "        # 初始化网格搜索\n",
    "        grid = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=config.get('grid_params', {}),\n",
    "            scoring='accuracy',\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # 执行搜索\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        # 保存结果\n",
    "        improved_models[model_name] = {\n",
    "            'best_model': grid.best_estimator_,\n",
    "            'best_params': grid.best_params_,\n",
    "            'best_score': grid.best_score_,\n",
    "            'grid_results': grid\n",
    "        }\n",
    "\n",
    "        # 可视化分析\n",
    "        plot_grid_search_results(improved_models[model_name], model_name)\n",
    "\n",
    "    return improved_models\n",
    "\n",
    "\n",
    "# 初始化基准模型\n",
    "base_models = {name: config['class'](**config['params'])\n",
    "               for name, config in MODEL_CONFIG.items()}\n",
    "\n",
    "# 优化模型\n",
    "improved_models = improve_models_via_gridsearch(\n",
    "    base_models,\n",
    "    MODEL_CONFIG,\n",
    "    X_train_scaled,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# 保存最佳模型\n",
    "for name, data in improved_models.items():\n",
    "    joblib.dump(data['best_model'], f'best_{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ff65f-21c9-4de9-b647-e19b75d51078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
