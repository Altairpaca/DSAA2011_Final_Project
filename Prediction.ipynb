{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5917dc-3463-4474-8b5a-5eb38a63bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             roc_curve, auc, make_scorer, f1_score)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4a3c2-89e4-48aa-8c95-d51909c7aabf",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcc7c7-768f-4ce8-bab6-cf5301c6dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Keep the first row, which are the titles\n",
    "X_train_df = pd.read_csv('X_train.csv', header=0)\n",
    "X_test_df = pd.read_csv('X_test.csv', header=0)\n",
    "X_train = X_train_df.values  # Convert to numpy array\n",
    "X_test = X_test_df.values\n",
    "# Extract Series from single-column DataFrame\n",
    "y_train = pd.read_csv('y_train.csv', header=0).squeeze()\n",
    "y_test = pd.read_csv(\"y_test.csv\", header=0).squeeze()\n",
    "\n",
    "# Save the list of feature names\n",
    "feature_columns = X_train_df.columns.tolist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c632631-a995-4b7a-8f8c-ae5f6f712941",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b7814-1ee9-4000-bf37-c01b24e79090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the label to start from 0 to meet the requirement of XGBoost\n",
    "min_label = min(y_train.min(), y_test.min())\n",
    "if min_label > 0:\n",
    "    y_train -= min_label\n",
    "    y_test -= min_label\n",
    "    print(f\"Adjusted the label to start from 0: {np.unique(y_train)}\")\n",
    "\n",
    "# Standardize to improve the model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa72118-8edb-4cc7-9c17-2eabf48d064a",
   "metadata": {},
   "source": [
    "### 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c16fc-17b8-4d70-9805-073e87d825f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    'LogisticRegression': {\n",
    "        'class': LogisticRegression,\n",
    "        'params': {'max_iter': 1000, 'class_weight': 'balanced'}\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'class': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 200, 'max_depth': 10, 'class_weight': 'balanced'}\n",
    "    },\n",
    "    'SVM': {\n",
    "        'class': SVC,\n",
    "        'params': {'kernel': 'rbf', 'C': 1.0, 'class_weight': 'balanced'}\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'class': XGBClassifier,\n",
    "        'params': {'eval_metric': 'logloss'}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    trained_models = {}\n",
    "    for model_name, config in MODEL_CONFIG.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # model initialization\n",
    "        model = config['class'](**config['params'])\n",
    "\n",
    "        # model training\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # record the model date\n",
    "        trained_models[model_name] = {\n",
    "            'model': model,\n",
    "            'train_time': time.time() - start_time,\n",
    "            'feature_importances': getattr(model, 'feature_importances_', None)\n",
    "        }\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "trained_models = train_models(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3060b1f-276b-4e52-a38e-151cd7d6623c",
   "metadata": {},
   "source": [
    "### 4. Prediction and Evaluation\n",
    "##### Visualize the confusion matrix, and calculate the accuracy/ precision/ recall/ f1 score for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aed086-3902-41e6-9a32-7b9a93206879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_sets(model, X_train, X_test):\n",
    "    # Test the trained model on the training set, testing set and the entire set\n",
    "    X_full = np.vstack([X_train, X_test])\n",
    "    return {\n",
    "        'train': model.predict(X_train),\n",
    "        'test': model.predict(X_test),\n",
    "        'full': model.predict(X_full)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name, model_name):\n",
    "    # Generate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "    plt.title(f'{model_name} - {dataset_name} Set Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compare_models(results):\n",
    "    comparison = pd.DataFrame()\n",
    "    for model_name, data in results.items():\n",
    "        metrics = {\n",
    "            'Train Accuracy': data['train']['accuracy'],\n",
    "            'Test Accuracy': data['test']['accuracy'],\n",
    "            'Full Accuracy': data['full']['accuracy'],\n",
    "            'Train Precision': data['train']['precision'],\n",
    "            'Test Precision': data['test']['precision'],\n",
    "            'Full Precision': data['full']['precision'],\n",
    "            'Train Recall': data['train']['recall'],\n",
    "            'Test Recall': data['test']['recall'],\n",
    "            'Full Recall': data['full']['recall'],\n",
    "            'Train F1': data['train']['f1'],\n",
    "            'Test F1': data['test']['f1'],\n",
    "            'Full F1': data['full']['f1']\n",
    "        }\n",
    "        comparison[model_name] = pd.Series(metrics)\n",
    "    return comparison.T\n",
    "\n",
    "\n",
    "# Prediction and evaluation flow\n",
    "results = {}\n",
    "for model_name, model_data in trained_models.items():\n",
    "    # Get predictions using scaled data\n",
    "    preds = predict_all_sets(\n",
    "        model_data['model'], X_train_scaled, X_test_scaled)\n",
    "\n",
    "    # Create full dataset labels\n",
    "    y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "    # Evaluate all sets with visualization\n",
    "    results[model_name] = {\n",
    "        'train': evaluate_model(y_train, preds['train'], 'Train', model_name),\n",
    "        'test': evaluate_model(y_test, preds['test'], 'Test', model_name),\n",
    "        'full': evaluate_model(y_full, preds['full'], 'Full', model_name)\n",
    "    }\n",
    "\n",
    "# Generate comparison table with full set metrics\n",
    "comparison_table = compare_models(results)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "# ROC and AUC\n",
    "# Modify SVM configuration to enable probability estimates\n",
    "MODEL_CONFIG['SVM']['params']['probability'] = True\n",
    "\n",
    "# Function to plot ROC curves and compute AUC for each model\n",
    "\n",
    "\n",
    "def plot_roc_and_calculate_auc(trained_models, X_test_scaled, y_test):\n",
    "    for model_name, model_data in trained_models.items():\n",
    "        model = model_data['model']\n",
    "\n",
    "        # Skip models without probability estimates\n",
    "        if not hasattr(model, 'predict_proba'):\n",
    "            print(\n",
    "                f\"Skipping ROC/AUC for {model_name} (no probability support)\")\n",
    "            continue\n",
    "\n",
    "        # Get model's class information\n",
    "        model_classes = model.classes_\n",
    "        n_classes = len(model_classes)\n",
    "\n",
    "        # Binarize test labels according to model's classes\n",
    "        y_test_bin = label_binarize(y_test, classes=model_classes)\n",
    "\n",
    "        # Get predicted probabilities\n",
    "        y_prob = model.predict_proba(X_test_scaled)\n",
    "\n",
    "        # Initialize structures for ROC/AUC\n",
    "        fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "        # Calculate metrics per class\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute macro-average\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Compute micro-average\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(\n",
    "            y_test_bin.ravel(), y_prob.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))\n",
    "\n",
    "        # Plot individual classes\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                     label=f'Class {model_classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "        # Plot averages\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.2f})',\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.2f})',\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "        # Formatting\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'Multi-class ROC for {model_name}')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"\\n{model_name} AUC Summary:\")\n",
    "        for i in range(n_classes):\n",
    "            print(f\"  Class {model_classes[i]}: {roc_auc[i]:.2f}\")\n",
    "        print(f\"  Macro-Average: {roc_auc['macro']:.2f}\")\n",
    "        print(f\"  Micro-Average: {roc_auc['micro']:.2f}\")\n",
    "\n",
    "\n",
    "# Execute ROC/AUC analysis\n",
    "plot_roc_and_calculate_auc(trained_models, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6021f-90bd-4131-8552-f3fd2a957955",
   "metadata": {},
   "source": [
    "### 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c52b3d-6fdf-4c78-9e2b-1ad1bc59a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix feature_columns definition (correct the typo)\n",
    "feature_columns = X_train_df.columns.tolist()\n",
    "\n",
    "# Visualize Decision Boundaries using PCA\n",
    "\n",
    "\n",
    "def plot_decision_boundaries(trained_models, X_train, X_test, y_train):\n",
    "    # Combine train and test for full PCA fit\n",
    "    X_full = np.vstack([X_train, X_test])\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X_full)\n",
    "\n",
    "    # Transform both train and test\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    for model_name, model_data in trained_models.items():\n",
    "        model = model_data['model']\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Predict on PCA meshgrid\n",
    "        Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot decision boundary\n",
    "        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "        # Plot training data points\n",
    "        scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train,\n",
    "                              edgecolor='k', cmap=plt.cm.RdYlBu, s=50)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.title(f'Decision Boundary for {model_name}\\n(PCA Projection)')\n",
    "        plt.legend(*scatter.legend_elements(), title='Classes')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize Feature Importances\n",
    "\n",
    "\n",
    "def plot_feature_importances(trained_models, feature_columns):\n",
    "    for model_name, model_data in trained_models.items():\n",
    "        importances = model_data['feature_importances']\n",
    "        if importances is not None:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
    "            plt.title(f\"Feature Importances - {model_name}\")\n",
    "            plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "            plt.xticks(range(len(indices)), [feature_columns[i] for i in indices],\n",
    "                       rotation=45, ha='right')\n",
    "            plt.xlim([-0.5, len(indices)-0.5])\n",
    "            plt.ylabel(\"Importance Score\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Execute visualization functions\n",
    "print(\"\\nGenerating Visualizations...\")\n",
    "plot_decision_boundaries(trained_models, X_train, X_test, y_train)\n",
    "plot_feature_importances(trained_models, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc038db2-561c-46af-b2c0-bcf780cdf163",
   "metadata": {},
   "source": [
    "### 6. Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbd784-429f-4861-b105-21a98c37948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 定义网格搜索参数空间\n",
    "MODEL_CONFIG = {\n",
    "    'LogisticRegression': {\n",
    "        'class': LogisticRegression,\n",
    "        'params': {\n",
    "            'max_iter': [1000],\n",
    "            'solver': ['lbfgs', 'sag'],\n",
    "            'C': [0.1, 1, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'class': RandomForestClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'class': SVC,\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 15],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'kernel': ['rbf', 'poly']\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'class': XGBClassifier,\n",
    "        'params': {\n",
    "            'learning_rate': [0.01, 0.1, 0.5],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def plot_grid_search_results(model_data, model_name):\n",
    "    results = model_data['cv_results']\n",
    "\n",
    "    # 提取关键参数\n",
    "    params = [p for p in results.columns if p.startswith('param_')]\n",
    "    top_params = results.loc[results['rank_test_f1'] <= 3, params].iloc[0]\n",
    "\n",
    "    # 创建热力图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    heatmap_data = results.pivot_table(\n",
    "        index=params[0],\n",
    "        columns=params[1] if len(params) > 1 else 'param_C',\n",
    "        values='mean_test_f1'\n",
    "    )\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n",
    "    plt.title(f'{model_name} Parameter Optimization (F1 Score)')\n",
    "    plt.show()\n",
    "\n",
    "    # 显示最佳参数\n",
    "    print(f\"\\nOptimal parameters: \")\n",
    "    for param, value in model_data['best_params'].items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    print(f\"Best f1 score in validation set: {model_data['best_score']:.4f}\")\n",
    "\n",
    "\n",
    "def improve_models_via_gridsearch(trained_models, MODEL_CONFIG, X_train, y_train):\n",
    "    \"\"\"使用网格搜索优化模型并可视化结果\"\"\"\n",
    "    improved_models = {}\n",
    "    scoring = {\n",
    "        'f1': make_scorer(f1_score, average='weighted'),\n",
    "        'accuracy': 'accuracy'\n",
    "    }\n",
    "\n",
    "    for model_name in trained_models:\n",
    "        print(f\"\\n=== Optimizing {model_name} ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 获取参数空间和模型类\n",
    "        param_grid = MODEL_CONFIG[model_name]['params']\n",
    "        model_class = MODEL_CONFIG[model_name]['class']\n",
    "\n",
    "        # 配置网格搜索\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model_class(),\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            refit='f1',\n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True),\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # 执行搜索\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        # 保存结果\n",
    "        improved_models[model_name] = {\n",
    "            'best_model': grid.best_estimator_,\n",
    "            'best_params': grid.best_params_,\n",
    "            'best_score': grid.best_score_,\n",
    "            'cv_results': pd.DataFrame(grid.cv_results_),\n",
    "            'search_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "        # 可视化参数影响\n",
    "        plot_grid_search_results(improved_models[model_name], model_name)\n",
    "\n",
    "    return improved_models\n",
    "\n",
    "\n",
    "# 初始化基准模型\n",
    "base_models = {name: config['class'](**config['params'])\n",
    "               for name, config in MODEL_CONFIG.items()}\n",
    "\n",
    "# 优化模型\n",
    "improved_models = improve_models_via_gridsearch(\n",
    "    base_models, MODEL_CONFIG, X_train, y_train)\n",
    "\n",
    "# 保存最佳模型\n",
    "for name, data in improved_models.items():\n",
    "    joblib.dump(data['best_model'], f'best_{name}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
